{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a Decision Tree \n",
    "\n",
    "\n",
    "The problem of constructing a decision tree can be expressed recursively. First, select an attribute to place at the root node and make one branch for each possible value. This splits up the example set into subsets, one for every value of the attribute. \n",
    "\n",
    "Now the process can be repeated recursively for each branch, using only those instances that actually reach the branch. If at any time all instances at a node have the same classification, stop developing that part of the tree.\n",
    "\n",
    "The only thing left to decide is how to determine which attribute to split on,\n",
    "given a set of examples with different classes.Consider the weather data. \n",
    "\n",
    "![](img/weather-data.png)\n",
    "\n",
    "There are four possibilities for each split.\n",
    "\n",
    "![](img/tree-top.png)\n",
    "\n",
    "Which is the best choice? The number of yes and no classes are shown at the leaves. Any leaf with only one class—yes or no—will not have to be split further, and the recursive process down that branch will terminate.\n",
    "\n",
    "Because we seek *small trees*, we would like this to happen as soon as possible. If we had a measure of the **purity** of each node, we could choose the attribute that produces the purest daughter nodes. A pure node would be one where there are only examples belonging to only a single class. \n",
    "\n",
    "The measure of purity that we will use is called the information entropy and is measured in units called bits. How do we calculate this???\n",
    "\n",
    "We calculate it based on the number of yes and no classes at the node. When evaluating the first tree (outlook tree), the numbers of yes and no classes at the leaf nodes are [2,3], [4,0], and [3,2].\n",
    "\n",
    "We calculate the information gain for each attribute and split on the one that gains the most information.\n",
    "\n",
    "\n",
    "The information we calculate has these properties:\n",
    "1. When the number of either yes’s or no’s is zero, the information is zero.\n",
    "2. When the number of yes’s and no’s is equal, the information reaches a maximum.\n",
    "\n",
    "Information value or entropy is calculated as \n",
    "\n",
    "$\n",
    "entropy(p_1,p_2, .. p_n) = -p_1 log_2 p_1 - p_2 log_2 p_2,  ... - p_n log_2 p_n\n",
    "$\n",
    "\n",
    "The Outlook attribute has three values, sunny, overcast, rainy. The sunny value yields 2 positive and 3 negative classification result if we used this attribute for classification. \n",
    "\n",
    "\n",
    "\n",
    "Reference: \n",
    "- *Witten & Frank. 2005. Data Mining Practical Machine Learning Tools and Techniques, 2nd Ed. Morgan Kaufman*\n",
    "- *Quinlan, J. R. 1986. Induction of Decision Trees. Mach. Learn. 1, 1 (Mar. 1986), 81–106*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy (sunny): 0.9709505944546688\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "sunny_info=entropy([2/5,3/5], base=2)\n",
    "print('Entropy (sunny):', sunny_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that calculated information values of these nodes for Outlook are:\n",
    "\n",
    "sunny_info([2,3])=0.971 bits\n",
    "overcast_info([4,0])=0.0 bits\n",
    "rainy_info([3,2)]=0.971 bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating the entropy for each attribute value, we obtain the average entropy for the node Outlook. Outlook([2+,3-], [4+,0-], [3+,2-]) is given as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6935714285714286"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Outlook=5./14. * 0.971 + 4./14. * 0 +  5./14. * 0.971\n",
    "Outlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the **Gain** to decide which node is used to split the tree. To do that we need to calculate the **Gain** for all possible splits (all attributes)\n",
    "\n",
    "We calculate the *gain differences* before and after the split.  The entropy of the problem before any tree is split is the entropy of 9+ and 5-, corresponding to:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.940285958670631"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree=entropy([9./14.,5./14.], base=2)\n",
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The change or the information gain obtained after the split at Outlook is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gain(outlook): 0.2467145300992024\n"
     ]
    }
   ],
   "source": [
    "gain_outlook=tree-Outlook\n",
    "print('Gain(outlook):', gain_outlook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gain(outlook) can be interpreted as the informational value of creating a branch on the outlook attribute.\n",
    "\n",
    "The gain for each attribute split can be calculated as:\n",
    "\n",
    "- gain(outlook) = 0.247 bits\n",
    "- gain(temperature) = 0.029 bits\n",
    "- gain(humidity) = 0.152 bits\n",
    "- gain(windy) = 0.048 bits\n",
    "\n",
    "Therefore, we select outlook as the splitting attribute at the root of the tree. Hopefully this accords with your intuition as the best one to select. It is the only choice for which one daughter node is completely pure, and this gives it a considerable advantage over the other attributes. Humidity is the next best choice because it produces a larger daughter node that is almost completely pure.\n",
    "\n",
    "### Question\n",
    "1. Verify the gain for temperature, humidity and windy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy (hot): 1.0\n",
      "Entropy (mild): 0.9182958340544894\n",
      "Entropy (cool): 0.8112781244591328\n"
     ]
    }
   ],
   "source": [
    "hot_info=entropy([2/4,2/4], base=2)\n",
    "mild_info=entropy([4/6,2/6], base=2)\n",
    "cool_info=entropy([3/4,1/4], base=2)\n",
    "\n",
    "print('Entropy (hot):', hot_info)\n",
    "print('Entropy (mild):', mild_info)\n",
    "print('Entropy (cool):', cool_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9110633930116763"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp=4/14 * 1 +  6/14 * 0.9182958340544894 +  4/14 * 0.8112781244591328\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.940285958670631"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_tree=entropy([9./14.,5./14.], base=2)\n",
    "temp_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gain(temp): 0.029222565658954758\n"
     ]
    }
   ],
   "source": [
    "gain_temp=temp_tree-temp\n",
    "print('Gain(temp):', gain_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy (high): 0.9852281360342515\n",
      "Entropy (normal): 0.5916727785823274\n"
     ]
    }
   ],
   "source": [
    "high_info=entropy([3/7,4/7], base=2)\n",
    "normal_info=entropy([6/7,1/7], base=2)\n",
    "print('Entropy (high):', high_info)\n",
    "print('Entropy (normal):', normal_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7884504573082894"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humidity=7./14. * 0.9852281360342515  +  7./14. * 0.5916727785823274\n",
    "humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.940285958670631"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humidity_tree=entropy([9./14.,5./14.], base=2)\n",
    "humidity_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gain(humidity): 0.15183550136234159\n"
     ]
    }
   ],
   "source": [
    "gain_humidity=humidity_tree-humidity\n",
    "print('Gain(humidity):', gain_humidity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "windy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy (false): 0.8112781244591328\n",
      "Entropy (true): 1.0\n"
     ]
    }
   ],
   "source": [
    "false_info=entropy([6/8,2/8], base=2)\n",
    "true_info=entropy([3/6,3/6], base=2)\n",
    "print('Entropy (false):', false_info)\n",
    "print('Entropy (true):', true_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8921589282623617"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windy=8./14. * 0.8112781244591328  +  6./14. * 1\n",
    "windy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.940285958670631"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windy_tree=entropy([9./14.,5./14.], base=2)\n",
    "windy_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gain(windy): 0.04812703040826938\n"
     ]
    }
   ],
   "source": [
    "gain_windy=windy_tree-windy\n",
    "print('Gain(windy):', gain_windy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree algorithms are based on a divide-and-conquer approach to the classification problem. They work top-down, seeking at each stage an attribute to split on that best separates the classes, and then recursively processing the subproblems that result from the split. This strategy generates a decision tree\n",
    "\n",
    "![](img/tree-2.png)\n",
    "\n",
    "Then we continue, recursively. The figure shows the possibilities for a further\n",
    "branch at the node reached when outlook is sunny. Clearly, a further split on\n",
    "outlook will produce nothing new, so we only consider the other three attributes.\n",
    "\n",
    "The information gain for each turns out to be :\n",
    "\n",
    "- gain(temperature)=0.571 bits\n",
    "- gain(humidity)=0.971 bits\n",
    "- gain(windy)=0.020 bits\n",
    "\n",
    "so we select humidity as the splitting attribute at this point. There is no need to split these nodes any further, so this branch is finished.\n",
    "\n",
    "Ideally, the process terminates when all leaf nodes are pure, that is, when they contain instances that all have the same classification.\n",
    "\n",
    "Finally we may have a decision tree as below:\n",
    "\n",
    "![](img/tree-3.png)\n",
    "\n",
    "### Question\n",
    "1. Verify the information gain for temperature, humidity and windy above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy (hot): 0.0\n",
      "Entropy (mild): 1.0\n",
      "Entropy (cool): 0.0\n"
     ]
    }
   ],
   "source": [
    "hot_info=entropy([0/2,2/2], base=2)\n",
    "mild_info=entropy([1/2,1/2], base=2)\n",
    "cool_info=entropy([0/1,1/1],base=2)\n",
    "\n",
    "print('Entropy (hot):', hot_info)\n",
    "print('Entropy (mild):', mild_info)\n",
    "print('Entropy (cool):', cool_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp=2/5 * 0 +  2/5 * 1 +  1/5 * 0\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546688"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_tree=entropy([2./5.,3./5.], base=2)\n",
    "temp_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gain(temp): 0.5709505944546688\n"
     ]
    }
   ],
   "source": [
    "gain_temp=temp_tree-temp\n",
    "print('Gain(temp):', gain_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy (hot): 0.0\n",
      "Entropy (mild): 0.0\n"
     ]
    }
   ],
   "source": [
    "hith_info=entropy([0/3,3/3], base=2)\n",
    "normal_info=entropy([2/2,0/2], base=2)\n",
    "\n",
    "print('Entropy (high):', hith_info)\n",
    "print('Entropy (mild):', normal_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humidity= 3/5 * 0 +  2/5 * 0\n",
    "humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546688"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humidity_tree=entropy([2./5.,3./5.], base=2)\n",
    "humidity_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gain(humidity): 0.9709505944546688\n"
     ]
    }
   ],
   "source": [
    "gain_humidity=humidity_tree-humidity\n",
    "print('Gain(humidity):', gain_humidity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "windy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy (false): 0.9182958340544894\n",
      "Entropy (true): 1.0\n"
     ]
    }
   ],
   "source": [
    "false_info=entropy([1/3,2/3], base=2)\n",
    "true_info=entropy([1/2,1/2], base=2)\n",
    "\n",
    "print('Entropy (false):', false_info)\n",
    "print('Entropy (true):', true_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9509775004326937"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windy=3/5 * 0.9182958340544894 +  2/5 * 1\n",
    "windy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546688"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windy_tree=entropy([2./5.,3./5.], base=2)\n",
    "windy_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gain(windy): 0.019973094021975113\n"
     ]
    }
   ],
   "source": [
    "gain_windy=windy_tree-windy\n",
    "print('Gain(windy):', gain_windy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
